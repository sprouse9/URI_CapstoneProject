{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d14fd5a2-55fd-4776-bd24-948d95039b03",
      "metadata": {
        "id": "d14fd5a2-55fd-4776-bd24-948d95039b03"
      },
      "source": [
        "## The first cell will only execute if you're using Google Colab AND have not cloned the repository yet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1e1ce71f-5741-4b43-bed9-992586467ef1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e1ce71f-5741-4b43-bed9-992586467ef1",
        "outputId": "b53f8da2-27cf-4332-dd41-78d3d8c2a88b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not running in Google Colab or repository already cloned.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Check if running in Google Colab\n",
        "colab_setup = \"google.colab\" in sys.modules\n",
        "\n",
        "repo_url = \"https://github.com/sprouse9/URI_CapstoneProject.git\"\n",
        "repo_name = \"URI_CapstoneProject\"\n",
        "\n",
        "# Clone only if running in Google Colab. Prevent cloning repo if already cloned.\n",
        "if colab_setup and not os.path.exists('../' + repo_name):\n",
        "    # Clone the repository if not already cloned\n",
        "\n",
        "    if not os.path.exists(repo_name):\n",
        "        print(f\"Cloning repository: {repo_url}...\")\n",
        "        !git clone {repo_url}\n",
        "\n",
        "    # Change directory to the repository\n",
        "    %cd {repo_name}\n",
        "else:\n",
        "    print(\"Not running in Google Colab or repository already cloned.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5caa44b7-c42e-412f-a2b5-70de03159230",
      "metadata": {
        "id": "5caa44b7-c42e-412f-a2b5-70de03159230"
      },
      "source": [
        "#### This next cell takes care of the dataset download from my Google Drive as a zip file.  \n",
        "#### The zip file will be auto extracted to your local machine or instance of Colab.\n",
        "#### The download will not occur if the zip file or the extracted folder already exists.\n",
        "#### The data folder 'archive' will not be unzipped again if already exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e9fa35aa-d415-48bf-a1a8-791d1a0383bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9fa35aa-d415-48bf-a1a8-791d1a0383bf",
        "outputId": "9c8404a3-be19-463b-ea9a-ba608786ed22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working dir: /content/URI_CapstoneProject\n",
            "Skipping download. CarDetectionDataSet.zip or archive already exists.\n",
            "Extracting CarDetectionDataSet.zip...\n",
            "Extraction complete: archive\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import importlib.util\n",
        "\n",
        "zip_filename = \"CarDetectionDataSet.zip\"\n",
        "extract_folder = \"archive\"  # Define the folder where files are extracted\n",
        "\n",
        "print(f\"Working dir: {os.getcwd()}\")  # This shows your current working directory\n",
        "\n",
        "# Download only if the zip file and extracted folder don't exist\n",
        "if not os.path.exists(zip_filename) and not os.path.exists(extract_folder):\n",
        "    # Check if gdown is installed before attempting to install\n",
        "    if importlib.util.find_spec(\"gdown\") is None:\n",
        "        print(\"gdown not found. Installing...\")\n",
        "        !pip install gdown\n",
        "\n",
        "    print(f\"{zip_filename} not found. Downloading...\")\n",
        "    !gdown 1JFAfrbUfXtiF-xwko2ACB-snDwIsj31h -O {zip_filename}\n",
        "else:\n",
        "    print(f\"Skipping download. {zip_filename} or {extract_folder} already exists.\")\n",
        "\n",
        "# Extract only if the extracted folder does not exist\n",
        "if not os.path.exists(extract_folder):\n",
        "    print(f\"Extracting {zip_filename}...\")\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "    print(f\"Extraction complete: {extract_folder}\")\n",
        "else:\n",
        "    print(f\"{extract_folder} already exists. Skipping extraction.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f7a1529-0d10-4711-82e9-f022f217e69a",
      "metadata": {
        "id": "3f7a1529-0d10-4711-82e9-f022f217e69a"
      },
      "source": [
        "## The data folder has been setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73d4b995-6598-4c5d-9303-5578ad25b537",
      "metadata": {
        "id": "73d4b995-6598-4c5d-9303-5578ad25b537"
      },
      "outputs": [],
      "source": [
        "if importlib.util.find_spec(\"ultralytics\") is None:\n",
        "    print(\"gdown not found. Installing...\")\n",
        "    !pip install ultralytics\n",
        "\n",
        "if importlib.util.find_spec(\"torch\") is None:\n",
        "    print(\"torch not found. Installing...\")\n",
        "    !pip install torch\n",
        "\n",
        "import torch\n",
        "print(\"PyTorch Version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "84048bf0-73db-4759-88e1-2481d643f27f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84048bf0-73db-4759-88e1-2481d643f27f",
        "outputId": "03a62b68-d32a-4e0f-ab5e-5ca6614de134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resolved dataset path: /content/URI_CapstoneProject/archive\n"
          ]
        }
      ],
      "source": [
        "# take care of library related path issues regardless of the\n",
        "# Operating system used\n",
        "\n",
        "import yaml\n",
        "\n",
        "# Load the existing dataset.yaml configuration\n",
        "with open('dataset.yaml', 'r') as f:\n",
        "    dataset = yaml.safe_load(f)\n",
        "\n",
        "# Get the current working directory\n",
        "cwd = os.getcwd()\n",
        "\n",
        "# Dynamically adjust the 'path'\n",
        "# This sets an absolute path for ultralytics so that it resolves correctly\n",
        "# Shouldn't be necessary but here we are\n",
        "dataset['path'] = os.path.join(cwd, 'archive')\n",
        "\n",
        "print(\"Resolved dataset path:\", dataset['path'])\n",
        "\n",
        "with open('dataset_updated.yaml', 'w') as f:\n",
        "    yaml.dump(dataset, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get GPU name and VRAM\n",
        "\n",
        "# Set the device: use \"cuda\" if available, otherwise \"cpu\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Optionally, get VRAM information if using a GPU\n",
        "if device == \"cuda\":\n",
        "    vram = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert bytes to GB\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"Using GPU ({gpu_name}) with {vram:.2f} GB VRAM\")\n",
        "else:\n",
        "    vram = None\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "#print(\"GPU Name:\", gpu_name)\n",
        "print(\"GPU VRAM:\", torch.cuda.get_device_properties(0).total_memory / (1024**3), \"GiB\")\n",
        "print(f\"Available VRAM: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "hwzhmD99k9dw",
        "outputId": "58f4bee3-f76b-4ceb-bbcb-5e06e9158488",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hwzhmD99k9dw",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU (Tesla T4) with 15.83 GB VRAM\n",
            "GPU VRAM: 14.74127197265625 GiB\n",
            "Available VRAM: 15.58 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88dc2625-ab0a-4522-8df1-3f04e49efefa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88dc2625-ab0a-4522-8df1-3f04e49efefa",
        "outputId": "d1baa3ae-0acf-4550-bfe8-17a96dfcc2d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VRAM is very large\n",
            "Training with lr: 0.0005, epochs: 10\n",
            "Ultralytics 8.3.77 ðŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=dataset_updated.yaml, epochs=10, time=None, patience=100, batch=64, imgsz=1024, save=True, save_period=-1, cache=disk, device=None, workers=8, project=runs/train/lr0.0005_ep10, name=train3, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.0005, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/train/lr0.0005_ep10/train3\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train/lr0.0005_ep10/train3', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/URI_CapstoneProject/archive/train/labels.cache... 400 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (2.3GB Disk): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:00<00:00, 34265.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/URI_CapstoneProject/archive/val/labels.cache... 99 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.6GB Disk): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:00<00:00, 31973.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/train/lr0.0005_ep10/train3/labels.jpg... \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from ultralytics import YOLO\n",
        "\n",
        "if device == \"cuda\":\n",
        "    learning_rates = [0.0005, 0.002, 0.01]\n",
        "    epochs_list = [10, 20, 50]\n",
        "    image_sz = 1024\n",
        "    # Using a tiered approach based on available VRAM:\n",
        "    if vram >= 15:\n",
        "        print(\"VRAM is very large\")\n",
        "        batch_sz = 64\n",
        "    elif vram > 10:\n",
        "        print(\"VRAM is large\")\n",
        "        batch_sz = 32  # a moderate increase\n",
        "    else:\n",
        "        batch_sz = 16\n",
        "else:\n",
        "    learning_rates = [0.0005, 0.002]\n",
        "    epochs_list = [10]\n",
        "    image_sz = 320\n",
        "    batch_sz = 4\n",
        "\n",
        "# List to store results from each experiment\n",
        "results_summary = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for epochs in epochs_list:\n",
        "        print(f\"Training with lr: {lr}, epochs: {epochs}\")\n",
        "\n",
        "        # Conditionally clear CUDA memory if available\n",
        "        if torch.cuda.is_available():\n",
        "          torch.cuda.empty_cache()\n",
        "          torch.cuda.ipc_collect()\n",
        "        else:\n",
        "          print(\"CUDA not available; skipping CUDA memory cleanup.\")\n",
        "\n",
        "        # Initialize the model\n",
        "        model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "        # Train the model with the current hyperparameters\n",
        "        results = model.train(\n",
        "            data=\"dataset_updated.yaml\",\n",
        "            epochs=epochs,\n",
        "            batch=batch_sz,\n",
        "            imgsz=image_sz,\n",
        "            lr0=lr,\n",
        "            #cache=True,\n",
        "            cache='disk',\n",
        "            optimizer=\"AdamW\",\n",
        "            project=f\"runs/train/lr{lr}_ep{epochs}\"\n",
        "        )\n",
        "\n",
        "        # Get the results dictionary\n",
        "        rdict = results.results_dict  # Contains keys like 'metrics/precision(B)' etc.\n",
        "\n",
        "        # Append the metrics along with the hyperparameters to our list\n",
        "        results_summary.append({\n",
        "            \"lr\": lr,\n",
        "            \"epochs\": epochs,\n",
        "            \"precision\": rdict.get(\"metrics/precision(B)\", None),\n",
        "            \"recall\": rdict.get(\"metrics/recall(B)\", None),\n",
        "            \"mAP50\": rdict.get(\"metrics/mAP50(B)\", None),\n",
        "            \"mAP50-95\": rdict.get(\"metrics/mAP50-95(B)\", None),\n",
        "            \"fitness\": rdict.get(\"fitness\", None)\n",
        "        })\n",
        "\n",
        "\n",
        "        # Conditionally clear CUDA memory if available\n",
        "        # if torch.cuda.is_available():\n",
        "        #    torch.cuda.empty_cache()\n",
        "        #    torch.cuda.ipc_collect()\n",
        "        # else:\n",
        "        #     print(\"CUDA not available; skipping CUDA memory cleanup.\")\n",
        "\n",
        "\n",
        "# Convert the results list into a DataFrame for easy viewing\n",
        "results_df = pd.DataFrame(results_summary)\n",
        "print(\"\\nBaseline Metrics for Each Test:\")\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58973bed-6e82-4468-9551-01c252b4ba09",
      "metadata": {
        "id": "58973bed-6e82-4468-9551-01c252b4ba09"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}