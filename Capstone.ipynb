{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa35aa-d415-48bf-a1a8-791d1a0383bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import importlib.util\n",
    "import subprocess\n",
    "\n",
    "zip_filename = \"CarDetectionDataSet.zip\"\n",
    "extract_folder = \"archive\"  # Define the folder where files are extracted\n",
    "\n",
    "print(f\"Working dir: {os.getcwd()}\")  # This shows your current working directory\n",
    "\n",
    "# Download only if the zip file and extracted folder don't exist\n",
    "if not os.path.exists(zip_filename) and not os.path.exists(extract_folder):\n",
    "    # Check if gdown is installed before attempting to install\n",
    "    if importlib.util.find_spec(\"gdown\") is None:\n",
    "        print(\"gdown not found. Installing...\")\n",
    "        !pip install gdown\n",
    "\n",
    "    print(f\"{zip_filename} not found. Downloading...\")\n",
    "    !gdown 1JFAfrbUfXtiF-xwko2ACB-snDwIsj31h -O {zip_filename}\n",
    "else:\n",
    "    print(f\"Skipping download. {zip_filename} or {extract_folder} already exists.\")\n",
    "\n",
    "# Extract only if the extracted folder does not exist\n",
    "if not os.path.exists(extract_folder):\n",
    "    print(f\"Extracting {zip_filename}...\")\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "    print(f\"Extraction complete: {extract_folder}\")\n",
    "else:\n",
    "    print(f\"{extract_folder} already exists. Skipping extraction.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a1529-0d10-4711-82e9-f022f217e69a",
   "metadata": {},
   "source": [
    "## The data folder has been setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4b995-6598-4c5d-9303-5578ad25b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "if importlib.util.find_spec(\"ultralytics\") is None:\n",
    "    print(\"gdown not found. Installing...\")\n",
    "    !pip install ultralytics\n",
    "\n",
    "if importlib.util.find_spec(\"torch\") is None:\n",
    "    print(\"torch not found. Installing...\")\n",
    "    !pip install torch\n",
    "\n",
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84048bf0-73db-4759-88e1-2481d643f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import yaml\n",
    "\n",
    "\n",
    "# Load the existing dataset.yaml configuration\n",
    "with open('dataset.yaml', 'r') as f:\n",
    "    dataset = yaml.safe_load(f)\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Dynamically adjust the 'path'\n",
    "# This sets an absolute path for ultralytics so that it resolves correctly\n",
    "# Shouldn't be necessary but here we are\n",
    "dataset['path'] = os.path.join(cwd, 'archive')\n",
    "\n",
    "print(\"Resolved dataset path:\", dataset['path'])\n",
    "\n",
    "# Optionally, save this updated configuration to a new file\n",
    "with open('dataset_updated.yaml', 'w') as f:\n",
    "    yaml.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc2625-ab0a-4522-8df1-3f04e49efefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Define hyperparameter grid\n",
    "learning_rates = [0.002] #[0.0005, 0.002, 0.01] #, 0.005, 0.01]\n",
    "epochs_list = [1] # , 20, 50] # , 15, 20, 25]\n",
    "\n",
    "# List to store results from each experiment\n",
    "results_summary = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for epochs in epochs_list:\n",
    "        print(f\"Training with lr: {lr}, epochs: {epochs}\")\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = YOLO(\"yolov8n.pt\")\n",
    "        \n",
    "        # Train the model with the current hyperparameters\n",
    "        results = model.train(\n",
    "            data=\"dataset_updated.yaml\",   # replace with your dataset YAML file path\n",
    "            epochs=epochs,\n",
    "            batch=8,\n",
    "            imgsz=128,\n",
    "            lr0=lr,\n",
    "            #cache=True,\n",
    "            cache='disk',\n",
    "            optimizer=\"AdamW\",\n",
    "            project=f\"runs/train/lr{lr}_ep{epochs}\"\n",
    "        )\n",
    "        \n",
    "        # Get the results dictionary\n",
    "        rdict = results.results_dict  # Contains keys like 'metrics/precision(B)' etc.\n",
    "        \n",
    "        # Append the metrics along with the hyperparameters to our list\n",
    "        results_summary.append({\n",
    "            \"lr\": lr,\n",
    "            \"epochs\": epochs,\n",
    "            \"precision\": rdict.get(\"metrics/precision(B)\", None),\n",
    "            \"recall\": rdict.get(\"metrics/recall(B)\", None),\n",
    "            \"mAP50\": rdict.get(\"metrics/mAP50(B)\", None),\n",
    "            \"mAP50-95\": rdict.get(\"metrics/mAP50-95(B)\", None),\n",
    "            \"fitness\": rdict.get(\"fitness\", None)\n",
    "        })\n",
    "\n",
    "        \n",
    "        # Conditionally clear CUDA memory if available\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "        else:\n",
    "            print(\"CUDA not available; skipping CUDA memory cleanup.\")\n",
    "\n",
    "\n",
    "# Convert the results list into a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "print(\"\\nBaseline Metrics for Each Test:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b76f9c-7564-46dc-a7c2-ea192497a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Load dataset.yaml\n",
    "with open('dataset.yaml') as f:\n",
    "    dataset = yaml.safe_load(f)\n",
    "\n",
    "# Optionally, print the resolved absolute paths for debugging:\n",
    "dataset_root = os.path.join(cwd, dataset['path'])\n",
    "train_path = os.path.join(dataset_root, dataset['train'])\n",
    "val_path = os.path.join(dataset_root, dataset['val'])\n",
    "\n",
    "print(\"Training images path:\", train_path)\n",
    "print(\"Validation images path:\", val_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58973bed-6e82-4468-9551-01c252b4ba09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
