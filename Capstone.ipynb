{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14fd5a2-55fd-4776-bd24-948d95039b03",
   "metadata": {
    "id": "d14fd5a2-55fd-4776-bd24-948d95039b03"
   },
   "source": [
    "## The first cell will only execute if you're using Google Colab AND have not cloned the repository yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e1ce71f-5741-4b43-bed9-992586467ef1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e1ce71f-5741-4b43-bed9-992586467ef1",
    "outputId": "1d2f59ef-3c5b-4776-834c-b1a20dd0b060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning repository: https://github.com/sprouse9/URI_CapstoneProject.git...\n",
      "Cloning into 'URI_CapstoneProject'...\n",
      "remote: Enumerating objects: 1455, done.\u001b[K\n",
      "remote: Counting objects: 100% (382/382), done.\u001b[K\n",
      "remote: Compressing objects: 100% (334/334), done.\u001b[K\n",
      "remote: Total 1455 (delta 40), reused 355 (delta 23), pack-reused 1073 (from 2)\u001b[K\n",
      "Receiving objects: 100% (1455/1455), 38.02 MiB | 34.51 MiB/s, done.\n",
      "Resolving deltas: 100% (56/56), done.\n",
      "/content/URI_CapstoneProject\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Check if running in Google Colab\n",
    "colab_setup = \"google.colab\" in sys.modules\n",
    "\n",
    "repo_url = \"https://github.com/sprouse9/URI_CapstoneProject.git\"\n",
    "repo_name = \"URI_CapstoneProject\"\n",
    "\n",
    "# Clone only if running in Google Colab. Prevent cloning repo if already cloned.\n",
    "if colab_setup and not os.path.exists('../' + repo_name):\n",
    "    # Clone the repository if not already cloned\n",
    "\n",
    "    if not os.path.exists(repo_name):\n",
    "        print(f\"Cloning repository: {repo_url}...\")\n",
    "        !git clone {repo_url}\n",
    "\n",
    "    # Change directory to the repository\n",
    "    %cd {repo_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caa44b7-c42e-412f-a2b5-70de03159230",
   "metadata": {
    "id": "5caa44b7-c42e-412f-a2b5-70de03159230"
   },
   "source": [
    "#### This next cell takes care of the dataset download from my Google Drive as a zip file.  \n",
    "#### The zip file will be auto extracted to your local machine or instance of Colab.\n",
    "#### The download will not occur if the zip file or the extracted folder already exists.\n",
    "#### The data folder 'archive' will not be unzipped again if already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9fa35aa-d415-48bf-a1a8-791d1a0383bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9fa35aa-d415-48bf-a1a8-791d1a0383bf",
    "outputId": "71c9a3f8-d150-458d-e1f3-0f23aa4e1a1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: /content/URI_CapstoneProject\n",
      "Skipping download. CarDetectionDataSet.zip or archive already exists.\n",
      "Extracting CarDetectionDataSet.zip...\n",
      "Extraction complete: archive\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import importlib.util\n",
    "\n",
    "zip_filename = \"CarDetectionDataSet.zip\"\n",
    "extract_folder = \"archive\"  # Define the folder where files are extracted\n",
    "\n",
    "print(f\"Working dir: {os.getcwd()}\")  # This shows your current working directory\n",
    "\n",
    "# Download only if the zip file and extracted folder don't exist\n",
    "if not os.path.exists(zip_filename) and not os.path.exists(extract_folder):\n",
    "    # Check if gdown is installed before attempting to install\n",
    "    if importlib.util.find_spec(\"gdown\") is None:\n",
    "        print(\"gdown not found. Installing...\")\n",
    "        !pip install gdown\n",
    "\n",
    "    print(f\"{zip_filename} not found. Downloading...\")\n",
    "    !gdown 1JFAfrbUfXtiF-xwko2ACB-snDwIsj31h -O {zip_filename}\n",
    "else:\n",
    "    print(f\"Skipping download. {zip_filename} or {extract_folder} already exists.\")\n",
    "\n",
    "# Extract only if the extracted folder does not exist\n",
    "if not os.path.exists(extract_folder):\n",
    "    print(f\"Extracting {zip_filename}...\")\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "    print(f\"Extraction complete: {extract_folder}\")\n",
    "else:\n",
    "    print(f\"{extract_folder} already exists. Skipping extraction.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a1529-0d10-4711-82e9-f022f217e69a",
   "metadata": {
    "id": "3f7a1529-0d10-4711-82e9-f022f217e69a"
   },
   "source": [
    "## The data folder has been setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73d4b995-6598-4c5d-9303-5578ad25b537",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73d4b995-6598-4c5d-9303-5578ad25b537",
    "outputId": "e54a64ae-b902-4c14-fc43-a8ec6747e932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdown not found. Installing...\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m921.5/921.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hPyTorch Version: 2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "if importlib.util.find_spec(\"ultralytics\") is None:\n",
    "    print(\"gdown not found. Installing...\")\n",
    "    !pip install -q ultralytics\n",
    "\n",
    "if importlib.util.find_spec(\"torch\") is None:\n",
    "    print(\"torch not found. Installing...\")\n",
    "    !pip install -q torch\n",
    "\n",
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84048bf0-73db-4759-88e1-2481d643f27f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84048bf0-73db-4759-88e1-2481d643f27f",
    "outputId": "ec81beca-0c1d-46c6-d312-6c4743821233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved dataset path: /content/URI_CapstoneProject/archive\n"
     ]
    }
   ],
   "source": [
    "# take care of library related path issues regardless of the\n",
    "# Operating system used\n",
    "\n",
    "import yaml\n",
    "\n",
    "# Load the existing dataset.yaml configuration\n",
    "with open('dataset.yaml', 'r') as f:\n",
    "    dataset = yaml.safe_load(f)\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Dynamically adjust the 'path'\n",
    "# This sets an absolute path for ultralytics so that it resolves correctly\n",
    "# Shouldn't be necessary but here we are\n",
    "dataset['path'] = os.path.join(cwd, 'archive')\n",
    "\n",
    "print(\"Resolved dataset path:\", dataset['path'])\n",
    "\n",
    "with open('dataset_updated.yaml', 'w') as f:\n",
    "    yaml.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hwzhmD99k9dw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwzhmD99k9dw",
    "outputId": "5fd0cd2a-9f48-463a-ab5a-aea1eed0503d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU (Tesla T4) with 15.83 GB VRAM\n",
      "GPU VRAM: 14.74127197265625 GiB\n",
      "Available VRAM: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Get GPU name and VRAM\n",
    "\n",
    "# Set the device: use \"cuda\" if available, otherwise \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Optionally, get VRAM information if using a GPU\n",
    "if device == \"cuda\":\n",
    "    vram = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert bytes to GB\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"Using GPU ({gpu_name}) with {vram:.2f} GB VRAM\")\n",
    "else:\n",
    "    vram = None\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "#print(\"GPU Name:\", gpu_name)\n",
    "print(\"GPU VRAM:\", torch.cuda.get_device_properties(0).total_memory / (1024**3), \"GiB\")\n",
    "print(f\"Available VRAM: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc2625-ab0a-4522-8df1-3f04e49efefa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88dc2625-ab0a-4522-8df1-3f04e49efefa",
    "outputId": "aa88ffe6-95b3-48e6-ef80-461c508ecdde"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "\n",
    "if device == \"cuda\":\n",
    "    learning_rates = [0.0005, 0.002, 0.01]\n",
    "    epochs_list = [10, 20, 50]\n",
    "    image_sz = 640\n",
    "    # Using a tiered approach based on available VRAM:\n",
    "    if vram >= 15:\n",
    "        print(\"VRAM is very large\")\n",
    "        batch_sz = 64\n",
    "    elif vram > 10:\n",
    "        print(\"VRAM is large\")\n",
    "        batch_sz = 32  # a moderate increase\n",
    "    else:\n",
    "        batch_sz = 16\n",
    "else:\n",
    "    learning_rates = [0.0005, 0.002]\n",
    "    epochs_list = [10]\n",
    "    image_sz = 320\n",
    "    batch_sz = 4\n",
    "\n",
    "# List to store results from each experiment\n",
    "results_summary = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for epochs in epochs_list:\n",
    "        print(f\"Training with lr: {lr}, epochs: {epochs}\")\n",
    "\n",
    "        # Conditionally clear CUDA memory if available\n",
    "        if torch.cuda.is_available():\n",
    "          torch.cuda.empty_cache()\n",
    "          torch.cuda.ipc_collect()\n",
    "        else:\n",
    "          print(\"CUDA not available; skipping CUDA memory cleanup.\")\n",
    "\n",
    "        # Initialize the model\n",
    "        model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "        # Train the model with the current hyperparameters\n",
    "        results = model.train(\n",
    "            data=\"dataset_updated.yaml\",\n",
    "            epochs=epochs,\n",
    "            batch=batch_sz,\n",
    "            imgsz=image_sz,\n",
    "            lr0=lr,\n",
    "            #cache=True,\n",
    "            cache='disk',\n",
    "            optimizer=\"AdamW\",\n",
    "            project=f\"runs/train/lr{lr}_ep{epochs}\"\n",
    "        )\n",
    "\n",
    "        # Get the results dictionary\n",
    "        rdict = results.results_dict  # Contains keys like 'metrics/precision(B)' etc.\n",
    "\n",
    "        # Append the metrics along with the hyperparameters to our list\n",
    "        results_summary.append({\n",
    "            \"lr\": lr,\n",
    "            \"epochs\": epochs,\n",
    "            \"precision\": rdict.get(\"metrics/precision(B)\", None),\n",
    "            \"recall\": rdict.get(\"metrics/recall(B)\", None),\n",
    "            \"mAP50\": rdict.get(\"metrics/mAP50(B)\", None),\n",
    "            \"mAP50-95\": rdict.get(\"metrics/mAP50-95(B)\", None),\n",
    "            \"fitness\": rdict.get(\"fitness\", None)\n",
    "        })\n",
    "\n",
    "\n",
    "        # Conditionally clear CUDA memory if available\n",
    "        # if torch.cuda.is_available():\n",
    "        #    torch.cuda.empty_cache()\n",
    "        #    torch.cuda.ipc_collect()\n",
    "        # else:\n",
    "        #     print(\"CUDA not available; skipping CUDA memory cleanup.\")\n",
    "\n",
    "\n",
    "# Convert the results list into a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "print(\"\\nBaseline Metrics for Each Test:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e43fb-6aca-40ad-87c7-ffe198800757",
   "metadata": {},
   "source": [
    "### Baseline Metrics for Each Test:\n",
    "\n",
    "|   lr   | epochs | precision |  recall  |   mAP50   | mAP50-95 | fitness |\n",
    "|--------|---------|-----------|----------|-----------|-----------|----------|\n",
    "| 0.0005 |    10   | 0.928012  | 0.562112 | 0.821413  | 0.609509  | 0.630699 |\n",
    "| 0.0005 |    20   | 0.849446  | 0.784161 | 0.892621  | 0.694569  | 0.714374 |\n",
    "| 0.0005 |    50   | 0.829223  | 0.821429 | 0.907031  | 0.726589  | 0.744633 |\n",
    "| 0.0020 |    10   | 0.945445  | 0.618930 | 0.849143  | 0.664448  | 0.682917 |\n",
    "| 0.0020 |    20   | 0.832686  | 0.803705 | 0.895060  | 0.718127  | 0.735820 |\n",
    "| 0.0020 |    50   | 0.854076  | 0.808862 | 0.909397  | 0.750816  | 0.766674 |\n",
    "| 0.0100 |    10   | 0.870470  | 0.739130 | 0.862304  | 0.596275  | 0.622878 |\n",
    "| 0.0100 |    20   | 0.917089  | 0.754658 | 0.879957  | 0.705703  | 0.723128 |\n",
    "| 0.0100 |    50   | 0.949638  | 0.768634 | 0.927499  | 0.777226  | 0.792253 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c53f43-ee6b-43a5-8c4a-4de8f3931915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device set to:\", device)\n",
    "\n",
    "# Initialize the YOLOv8n model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Define a search space for hyperparameter tuning\n",
    "# You can add or remove parameters as needed.\n",
    "search_space = {\n",
    "    \"lr0\": (1e-5, 1e-1),          # Initial learning rate\n",
    "    \"degrees\": (0.0, 45.0),         # Rotation augmentation range\n",
    "    \"momentum\": (0.6, 0.98),        # Momentum for SGD\n",
    "    \"weight_decay\": (0.0, 0.001),   # L2 regularization\n",
    "    \"scale\": (0.0, 0.9)             # Scaling augmentation range\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning using the Tuner class\n",
    "# Note: 'data' should point to your dataset configuration file (e.g., coco8.yaml or a custom one)\n",
    "results = model.tune(\n",
    "    data=\"coco8.yaml\",     # Update this with your dataset YAML if needed\n",
    "    epochs=50,             # Train for 50 epochs per tuning iteration\n",
    "    iterations=300,        # Number of tuning iterations (adjust as necessary)\n",
    "    optimizer=\"AdamW\",     # Use the AdamW optimizer\n",
    "    space=search_space,    # The defined search space\n",
    "    plots=False,           # Skip plotting for faster tuning\n",
    "    save=False,            # Skip checkpointing to reduce overhead\n",
    "    val=False              # Skip validation until the final epoch for faster tuning\n",
    ")\n",
    "\n",
    "# Print out the best hyperparameters found\n",
    "print(\"Tuning complete. Best hyperparameters:\")\n",
    "print(results.best_hyperparameters)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
